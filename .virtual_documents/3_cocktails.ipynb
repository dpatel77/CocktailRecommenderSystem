





# !pip install scikit-surprise
# !pip install matplotlib seaborn
# !pip install --upgrade pandas numpy
# !pip install python-pptx
# !pip install sdv


# import sdv

# print(sdv.version.public)


# url = "https://dachang.github.io/CocktailViz/cocktailWheel.JSON"


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.metrics import mean_squared_error, pairwise
from sklearn.neighbors import NearestNeighbors
from scipy.sparse import csr_matrix
from scipy.stats import randint
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from surprise import Dataset, Reader, SVD, accuracy
from surprise.model_selection import PredefinedKFold
from sklearn.metrics.pairwise import cosine_similarity
import hashlib
# import tensorflow as tf
# from tensorflow.keras.models import Model
# from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate
# from tensorflow.keras.optimizers import Adam






# Load the dataset
cocktails_file_path = 'cocktails.csv'
cocktails_df = pd.read_csv(cocktails_file_path)

# Replace all cells with '-' with blank
cocktails_df.replace('-', None, inplace=True)

# Display the first few rows of the updated dataset
print(cocktails_df.columns)
print(cocktails_df.shape)
print(cocktails_df['Base Wine'].unique())
cocktails_df.head()





# # EDA on cocktails data
# def eda_cocktails(data):
#     print("Cocktails Data Info:")
#     print(data.info())
#     # Summary Statistics
#     print("\nCocktails Data Description:")
#     print(data.describe())

#     for column in data.columns:
#         plt.figure(figsize=(12, 6))
        
#         if data[column].dtype == 'object':
#             # Categorical column - Bar plot
#             data[column].value_counts().plot(kind='bar')
#             plt.title(f'Distribution of {column}')
#             plt.xlabel(column)
#             plt.ylabel('Frequency')
#         else:
#             # Numerical column - Scatter plot
#             plt.scatter(data.index, data[column])
#             plt.title(f'Scatter plot of {column}')
#             plt.xlabel('Index')
#             plt.ylabel(column)
        
#         plt.show()

# # Perform EDA
# eda_cocktails(cocktails_data)


### Correlation Analysis


# Plot correlation matrix
# Select only numerical columns
numerical_cols = cocktails_df.select_dtypes(include=[np.number])

# Compute the correlation matrix for numerical columns
corr_matrix = numerical_cols.corr()

# Plot the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()





# Since the cocktails recipes are set there would not be a case to fill in missing values. If the data was truly missing then we would be an associate to fill in the correct information





from sklearn.preprocessing import StandardScaler

# Scale numerical columns
numerical_cols = cocktails_df.select_dtypes(include=['number']).columns
print(numerical_cols)
scaler = StandardScaler()
cocktails_df[numerical_cols] = scaler.fit_transform(cocktails_df[numerical_cols])

cocktails_df.head()


cocktails_df['Name'].unique()





# One-Hot Encode categorical columns, except the cocktail name column
categorical_cols = cocktails_df.drop(columns='Name').select_dtypes(include=['object']).columns
print(categorical_cols)
cocktails_df = pd.get_dummies(cocktails_df, columns=categorical_cols)

# Convert boolean columns to integers
boolean_cols = cocktails_df.select_dtypes(include=['bool']).columns
cocktails_df[boolean_cols] = cocktails_df[boolean_cols].astype(int)

cocktails_df.head()


### Correlation Analysis


# Plot correlation matrix
plt.figure(figsize=(12, 8))
corr_matrix = cocktails_df.drop(columns = 'Name').corr()
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()





## Creating the train/val separately from testing data (similar to new data in production)

# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split

# cocktail_ids = {name: i for i, name in enumerate(cocktails_df['Name'].unique())}

# # Generate improved synthetic user ratings data
# def generate_synthetic_ratings(num_users, num_ratings):
#     np.random.seed(42)
#     user_ids = np.random.randint(1, num_users + 1, num_ratings)
#     cocktail_names = np.random.choice(cocktails_df['Name'].unique(), num_ratings)
#     user_ratings = np.random.normal(loc=3, scale=1, size=num_ratings).astype(int)
#     user_ratings = np.clip(user_ratings, 1, 5)  # Ensure ratings are between 1 and 5
#     synthetic_data = pd.DataFrame({
#         'user_id': user_ids,
#         'cocktail_name': cocktail_names,
#         'user_rating': user_ratings
#     })

#     synthetic_data['cocktail_id'] = synthetic_data['cocktail_name'].map(cocktail_ids)
    
#     return synthetic_data

# num_users_train_val = 20000  # Number of unique users for training and validation
# num_ratings_train_val = 100000  # Number of ratings for training and validation
# ratings_df_train_val = generate_synthetic_ratings(num_users_train_val, num_ratings_train_val)
# print(ratings_df_train_val.head(10))

# num_users_test = 10000  # Number of unique users for test (upsampling)
# num_ratings_test = 1000  # Number of ratings for test before upsampling
# ratings_df_test = generate_synthetic_ratings(10000, num_ratings_test)

# # Upsample test data to 10000
# ratings_df_test = ratings_df_test.sample(n=10000, replace=True, random_state=42)
# ratings_df_test['user_id'] = np.random.randint(5000, 15001, len(ratings_df_test))

# # Split the training/validation data into 90% train and 10% validation sets
# train_data, val_data = train_test_split(ratings_df_train_val, test_size=0.1, random_state=42)

# print(f"Train data size: {len(train_data)}")
# print(f"Validation data size: {len(val_data)}")
# print(f"Test data size: {len(ratings_df_test)}")

# # Display the first few rows of each set
# print("Training Data:")
# print(train_data.head())

# print("\nValidation Data:")
# print(val_data.head())

# print("\nTest Data:")
# print(ratings_df_test.head())


## Generating train/val/test from the same dataset

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Assuming 'cocktails_df' is predefined with cocktail names
cocktail_ids = {name: i for i, name in enumerate(cocktails_df['Name'].unique())}

# Generate synthetic user ratings data
def generate_synthetic_ratings(num_users, num_ratings):
    np.random.seed(42)
    # user_ids = np.random.randint(1, num_users + 1, num_ratings)
    user_ids = pd.Series(np.random.randint(1, num_users + 1, num_ratings)).astype(str)
    # user_ids = "User" + user_ids
    cocktail_names = np.random.choice(cocktails_df['Name'].unique(), num_ratings)
    user_ratings = np.random.normal(loc=3, scale=1, size=num_ratings).astype(int)
    user_ratings = np.clip(user_ratings, 1, 5)  # Ensure ratings are between 1 and 5
    synthetic_data = pd.DataFrame({
        'user_id': user_ids,
        'cocktail_name': cocktail_names,
        'user_rating': user_ratings
    })

    synthetic_data['cocktail_id'] = synthetic_data['cocktail_name'].map(cocktail_ids)
    synthetic_data['user_email'] = synthetic_data['user_id'].apply(lambda x: f'user{x}@gmail.com')
    
    return synthetic_data

num_users = 20000  # Number of unique users
num_ratings = 100000  # Total number of ratings

# Generate the data once
ratings_df = generate_synthetic_ratings(num_users, num_ratings)

ratings_df.to_csv('user_rating_test.csv', index=False)

# Split the data into 80% train, 10% validation, and 10% test sets
train_data, temp_data = train_test_split(ratings_df, test_size=0.2, random_state=42)
val_data, ratings_df_test = train_test_split(temp_data, test_size=0.5, random_state=42)

# Print sizes of the splits
print(f"Train data size: {len(train_data)}")
print(f"Validation data size: {len(val_data)}")
print(f"Test data size: {len(ratings_df_test)}")

# Display the first few rows of each set
print("Training Data:")
print(train_data.head())

print("\nValidation Data:")
print(val_data.head())

print("\nTest Data:")
print(ratings_df_test.head())


# # Using CTGan User Ratings, generated using colab: User_rating_CTGAN.ipynb

# # Assuming 'cocktails_df' is predefined with cocktail names
# cocktail_ids = {name: i for i, name in enumerate(cocktails_df['Name'].unique())}

# ratings_df = pd.read_csv('user_rating_CTGAN.csv')
# print(ratings_df.columns)
# print(ratings_df.head)

# ratings_df['cocktail_id'] = ratings_df['cocktail_name'].map(cocktail_ids)
# # ratings_df.drop(columns = 'user_email', inplace = True)
# print(ratings_df.groupby(['cocktail_name','cocktail_id'])['cocktail_id'].nunique())
      
# # Split the data into 80% train, 10% validation, and 10% test sets
# train_data, temp_data = train_test_split(ratings_df, test_size=0.2, random_state=42)
# val_data, ratings_df_test = train_test_split(temp_data, test_size=0.5, random_state=42)

# # Print sizes of the splits
# print(f"Train data size: {len(train_data)}")
# print(f"Validation data size: {len(val_data)}")
# print(f"Test data size: {len(ratings_df_test)}")

# # Display the first few rows of each set
# print("Training Data:")
# print(train_data.head())

# print("\nValidation Data:")
# print(val_data.head())

# print("\nTest Data:")
# print(ratings_df_test.head())


# Importing Ratings Data Raw

# Assuming 'cocktails_df' is predefined with cocktail names
cocktail_ids = {name: i for i, name in enumerate(cocktails_df['Name'].unique())}

# Load the CSV file
file_path = 'user_ratings_form.csv'
ratings_df = pd.read_csv(file_path)

ratings_df.drop(columns={'Timestamp','Your Name'}, inplace = True)
ratings_df.rename(columns={'Email':'user_id', 'Cocktail':'cocktail_name',
       'Rating (1=do not like, 5 = like very much)':'user_rating'}, inplace = True)

# Function to hash email addresses and convert to numerical value
def hash_email(email):
    hash_object = hashlib.sha256(email.encode())
    hex_dig = hash_object.hexdigest()
    return int(hex_dig, 16) % (10 ** 10)

# Apply the hashing function to the user_id column
ratings_df['user_id'] = ratings_df['user_id'].apply(hash_email)

# Display the first few rows of the data
print(ratings_df.head())
print(ratings_df.columns)

ratings_df['cocktail_id'] = ratings_df['cocktail_name'].map(cocktail_ids)
# ratings_df.drop(columns = 'user_email', inplace = True)
print(ratings_df.groupby(['cocktail_name','cocktail_id'])['cocktail_id'].nunique())
      
# Split the data into 80% train, 10% validation, and 10% test sets
train_data, temp_data = train_test_split(ratings_df, test_size=0.2, random_state=42)
val_data, ratings_df_test = train_test_split(temp_data, test_size=0.5, random_state=42)

# Print sizes of the splits
print(f"Train data size: {len(train_data)}")
print(f"Validation data size: {len(val_data)}")
print(f"Test data size: {len(ratings_df_test)}")

# Display the first few rows of each set
print("Training Data:")
print(train_data.head())

print("\nValidation Data:")
print(val_data.head())

print("\nTest Data:")
print(ratings_df_test.head())





# Apply SMOTE to balance the user rating distribution in the training set
# smote = SMOTE(random_state=42)
smote = SMOTE(random_state=42, k_neighbors=1)

# Separate features and target variable
X_train = train_data[['user_id', 'cocktail_id']]
y_train = train_data['user_rating']

# Apply SMOTE
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Combine resampled features and target variable into a new DataFrame
train_data_smote = pd.DataFrame(X_train_smote, columns=['user_id', 'cocktail_id'])
train_data_smote['user_rating'] = y_train_smote

print("\nTraining Data after SMOTE:")
print(train_data_smote.head())

# Check the distribution of ratings in the original and SMOTE datasets
print("\nOriginal Training Data Rating Distribution:")
print(train_data['user_rating'].value_counts())

print("\nSMOTE Training Data Rating Distribution:")
print(train_data_smote['user_rating'].value_counts())

print(f"Train data SMOTE size: {len(train_data_smote)}")
print(f"Validation data size: {len(val_data)}")
print(f"Test data size: {len(ratings_df_test)}")





# Apply Random Undersampling to the training data
rus = RandomUnderSampler(random_state=42)

# Separate features and target variable for undersampling
X_train = train_data[['user_id', 'cocktail_id']]
y_train = train_data['user_rating']

# Apply Random Undersampling
X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)

# Combine undersampled features and target variable into a new DataFrame
train_data_rus = pd.DataFrame(X_train_rus, columns=['user_id', 'cocktail_id'])
train_data_rus['user_rating'] = y_train_rus

print("\nTraining Data after Random Undersampling:")
print(train_data_rus.head())

# Check the distribution of ratings in the original and undersampled datasets
print("\nOriginal Training Data Rating Distribution:")
print(train_data['user_rating'].value_counts())

print("\nRandom Undersampling Training Data Rating Distribution:")
print(train_data_rus['user_rating'].value_counts())

print(f"Train data RUS size: {len(train_data_rus)}")
print(f"Validation data size: {len(val_data)}")
print(f"Test data size: {len(ratings_df_test)}")





# # Optionally, apply SMOTE after undersampling to balance the data further
# smote = SMOTE(random_state=42)
# X_train_smote, y_train_smote = smote.fit_resample(X_train_rus, y_train_rus)

# # Combine resampled features and target variable into a new DataFrame
# train_data_smote = pd.DataFrame(X_train_smote, columns=['user_id', 'cocktail_id'])
# train_data_smote['user_rating'] = y_train_smote

# print("\nTraining Data after SMOTE:")
# print(train_data_smote.head())

# # Check the distribution of ratings in the SMOTE dataset
# print("\nSMOTE Training Data Rating Distribution:")
# print(train_data_smote['user_rating'].value_counts())

# print(f"Train data RUS size: {len(train_data_rus)}")
# print(f"Validation data size: {len(val_data)}")
# print(f"Test data size: {len(ratings_df_test)}")





# Visualize the distribution


# Function to plot the distribution of ratings
def plot_rating_distribution(df, title):
    plt.figure(figsize=(12, 5))
    rating_counts = df['user_rating'].value_counts().sort_index()
    sns.barplot(x=rating_counts.index, y=rating_counts.values, palette='viridis')
    plt.xlabel('Rating')
    plt.ylabel('Count')
    plt.title(title)
    plt.show()

plot_rating_distribution(train_data, 'Training Data Rating Distribution')
plot_rating_distribution(train_data_smote, 'Training Data SMOTE Rating Distribution')
plot_rating_distribution(train_data_rus, 'Training Data RUS Rating Distribution')
plot_rating_distribution(val_data, 'Validation Data Rating Distribution')
plot_rating_distribution(ratings_df_test, 'Test Data Rating Distribution')


# Function to plot the distribution of cocktails
def plot_cocktail_distribution(df, title):
    plt.figure(figsize=(12, 5))
    cocktail_counts = df['cocktail_id'].value_counts().sort_index()
    sns.barplot(x=cocktail_counts.index, y=cocktail_counts.values, palette='viridis')
    plt.xlabel('Cocktail ID')
    plt.ylabel('Count')
    plt.title(title)
    plt.xticks(rotation=45)
    plt.show()

plot_cocktail_distribution(train_data, 'Top 10 Cocktails in Training Data')
plot_cocktail_distribution(train_data_smote, 'Top 10 Cocktails in Training Data SMOTE')
plot_cocktail_distribution(train_data_rus, 'Top 10 Cocktails in Training Data RUS')
plot_cocktail_distribution(val_data, 'Top 10 Cocktails in Validation Data')
plot_cocktail_distribution(ratings_df_test, 'Top 10 Cocktails in Test Data')


# # Encode cocktail names as IDs for plotting
# # cocktail_ids = {name: i for i, name in enumerate(cocktails_df['Name'].unique())}
# # train_data['cocktail_id'] = train_data['cocktail_name'].map(cocktail_ids)
# # val_data['cocktail_id'] = val_data['cocktail_name'].map(cocktail_ids)
# # ratings_df_test['cocktail_id'] = ratings_df_test['cocktail_name'].map(cocktail_ids)

# # Plotting function
# def plot_3d_data(data, title):
#     fig = plt.figure(figsize=(10, 8))
#     ax = fig.add_subplot(111, projection='3d')
#     sc = ax.scatter(data['user_id'], data['cocktail_id'], data['user_rating'], c=data['user_rating'], cmap='viridis')
#     ax.set_xlabel('User ID')
#     ax.set_ylabel('Cocktail ID')
#     ax.set_zlabel('User Rating')
#     ax.set_title(title)
#     plt.colorbar(sc, ax=ax, label='Rating')
#     plt.savefig(title)
#     plt.show()

# # Plot the training, validation, and test data
# plot_3d_data(train_data, '3D Visualization of Training Data')
# plot_3d_data(train_data_smote, '3D Visualization of Training Data SMOTE')
# plot_3d_data(train_data_rus, '3D Visualization of Training Data RUS')
# plot_3d_data(val_data, '3D Visualization of Validation Data')
# plot_3d_data(ratings_df_test, '3D Visualization of Test Data')

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Plotting function
def plot_3d_data(data, title):
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    sc = ax.scatter(data['user_id'], data['cocktail_id'], data['user_rating'], c=data['user_rating'], cmap='viridis')
    ax.set_xlabel('User ID')
    ax.set_ylabel('Cocktail ID')
    ax.set_zlabel('User Rating')
    ax.set_title(title)
    plt.colorbar(sc, ax=ax, label='Rating')
    
    # Hide x-axis tick labels
    ax.set_xticklabels([])
    
    plt.savefig(f'images/{title}.png')
    plt.show()

# Plot the training, validation, and test data
plot_3d_data(train_data, '3D Visualization of Training Data')
plot_3d_data(train_data_smote, '3D Visualization of Training Data SMOTE')
plot_3d_data(train_data_rus, '3D Visualization of Training Data RUS')
plot_3d_data(val_data, '3D Visualization of Validation Data')
plot_3d_data(ratings_df_test, '3D Visualization of Test Data')






# # from sklearn.ensemble import RandomForestRegressor

# # # # Prepare the data for feature importance calculation
# # # X = cocktails_df.drop(columns='Name')
# # # y = np.random.randint(1, 6, len(X))  # Dummy target variable for feature importance

# # # # Train a random forest model to compute feature importance
# # # rf = RandomForestRegressor()
# # # rf.fit(X, y)

# # # # Get feature importances
# # # importances = rf.feature_importances_
# # # feature_names = X.columns
# # # feature_importances = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values(by='importance', ascending=False)

# # # print(feature_importances.head(10))


# # # Merge the cocktails data with the ratings data on cocktail name
# # merged_df = ratings_df.merge(cocktails_df, left_on='cocktail_name', right_on='Name')

# # print(merged_df.dtypes)
# # print(merged_df.columns)

# # # Prepare the data for feature importance calculation
# # # Use user_rating as the target variable
# # X = merged_df.drop(columns=['user_rating', 'Name', 'cocktail_name', 'user_id'])
# # y = merged_df['user_rating']


# # # # Identify categorical and numerical features
# # # categorical_features = ['Category', 'Making', 'Base Wine', 'Liquor', 'Juice', 'Spice', 'Soda', 'Others', 'Taste', 'Type of Glass']
# # # numerical_features = ['Alcohol', 'Base Wine Amount', 'Liquor Amount', 'Juice Amount', 'Spice Amount', 'Soda Amount', 'Salty', 'Savory', 'Sour', 'Bitter', 'Sweet', 'Spicy']

# # categorical_features = ['cocktail_name']
# # numerical_features = merged_df.drop(columns=['cocktail_name']).columns

# # # Create a column transformer with one-hot encoding for categorical features
# # preprocessor = ColumnTransformer(
# #     transformers=[
# #         ('num', 'passthrough', numerical_features),
# #         ('cat', OneHotEncoder(), categorical_features)
# #     ])

# # # Pipeline with preprocessing and random forest regressor
# # model = Pipeline(steps=[
# #     ('preprocessor', preprocessor),
# #     ('regressor', RandomForestRegressor())
# # ])

# # # Train the model
# # model.fit(X, y)

# # # Extract the trained RandomForestRegressor from the pipeline
# # rf = model.named_steps['regressor']

# # # Get feature importances
# # importances = rf.feature_importances_
# # feature_names = model.named_steps['preprocessor'].get_feature_names_out()
# # feature_importances = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values(by='importance', ascending=False)

# # print(feature_importances.head(10))


# import pandas as pd
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.preprocessing import OneHotEncoder, StandardScaler
# from sklearn.compose import ColumnTransformer
# from sklearn.pipeline import Pipeline

# # Merge the cocktails data with the ratings data on cocktail name
# merged_df = ratings_df.merge(cocktails_df, left_on='cocktail_name', right_on='Name')

# # Print data types and columns to check the merge
# print(merged_df.dtypes)
# print(merged_df.columns)

# # Prepare the data for feature importance calculation
# # Use user_rating as the target variable
# X = merged_df.drop(columns=['user_rating', 'Name', 'cocktail_name', 'user_id'])
# y = merged_df['user_rating']

# # Identify categorical and numerical features
# categorical_features = X.select_dtypes(include=['object', 'bool']).columns.tolist()
# numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

# # Create a column transformer with one-hot encoding for categorical features
# preprocessor = ColumnTransformer(
#     transformers=[
#         ('num', StandardScaler(), numerical_features),
#         ('cat', OneHotEncoder(), categorical_features)
#     ])

# # Pipeline with preprocessing and random forest regressor
# model = Pipeline(steps=[
#     ('preprocessor', preprocessor),
#     ('regressor', RandomForestRegressor())
# ])

# # Train the model
# model.fit(X, y)

# # Extract the trained RandomForestRegressor from the pipeline
# rf = model.named_steps['regressor']

# # Get feature importances
# importances = rf.feature_importances_
# feature_names = model.named_steps['preprocessor'].get_feature_names_out()
# feature_importances = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values(by='importance', ascending=False)

# print(feature_importances.head(10))









# print(cocktails_df.columns)
# print(cocktails_df.dtypes)
# cocktails_df.head()


# Identify categorical and numerical columns
categorical_cols = cocktails_df.select_dtypes(include=['object', 'bool']).columns.difference(['Name'])
numerical_cols = cocktails_df.select_dtypes(include=[np.number]).columns

# Define the preprocessing steps for categorical and numerical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(), categorical_cols)
    ])

# Apply the transformations
transformed_features = preprocessor.fit_transform(cocktails_df)

# Compute the cosine similarity between all pairs of cocktails
similarity_matrix = cosine_similarity(transformed_features)

# Create a DataFrame to display similarities nicely
similarity_df = pd.DataFrame(similarity_matrix, index=cocktails_df['Name'], columns=cocktails_df['Name'])
# print(similarity_df)

# Convert DataFrame to Series and sort values in descending order
sorted_pairs = similarity_df.unstack().sort_values(ascending=False).dropna()
# print(sorted_pairs)

# Remove self-similarities by setting them to NaN
sorted_pairs[sorted_pairs == 1] = np.nan
sorted_pairs = sorted_pairs.dropna()

# # Map cocktail_id to cocktail_name in sorted_pairs
sorted_pairs.index = sorted_pairs.index.set_names(['Cocktail Name 1', 'Cocktail Name 2'])
sorted_pairs = sorted_pairs.reset_index()
sorted_pairs.rename(columns={0: 'Similarity Score'}, inplace=True)

# Remove entries with the same names
sorted_pairs = sorted_pairs[sorted_pairs['Cocktail Name 1'] != sorted_pairs['Cocktail Name 2']]

# Ensure each pair is considered only once by sorting the cocktail names within each pair
sorted_pairs[['Cocktail Name 1', 'Cocktail Name 2']] = np.sort(sorted_pairs[['Cocktail Name 1', 'Cocktail Name 2']], axis=1)

# Remove duplicate pairs
sorted_pairs = sorted_pairs.drop_duplicates(subset=['Cocktail Name 1', 'Cocktail Name 2'])

# Print the pairs in descending order of similarity score
print(sorted_pairs)

# Save the similarity DataFrame to a CSV file
similarity_df.to_csv('cocktail_similarity_df.csv', index=True)
sorted_pairs.to_csv('sorted_pairs.csv', index=False)





from sklearn.metrics.pairwise import cosine_similarity

# Compute cosine similarity between cocktails based on their features
cocktail_features = cocktails_df.set_index('Name')
# print(cocktail_features)
similarity_matrix = cosine_similarity(cocktail_features)
# print(similarity_matrix)

# Create a recommendation function
def recommend_cocktails(cocktail_name, similarity_matrix, cocktail_names, top_n):
    idx = cocktail_names.index(cocktail_name)
    # print(idx)
    similarity_scores = list(enumerate(similarity_matrix[idx]))
    # print(similarity_scores)
    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)
    # print(similarity_scores)
    recommended_cocktails = [cocktail_names[i[0]] for i in similarity_scores[1:top_n+1]]
    # print(recommended_cocktails)
    return recommended_cocktails

# Test the recommendation function
cocktail_names = cocktails_df['Name'].tolist()
cocktail_name = 'Margarita'  # Replace this with any cocktail from the dataset
num_of_recs = 5

recommended_cocktails = recommend_cocktails(cocktail_name, similarity_matrix, cocktail_names, num_of_recs)

# Print the recommended cocktails
print("Recommended cocktails similar to a", cocktail_name, ":")
print(recommended_cocktails)









import pandas as pd
from surprise import Dataset, Reader
from surprise import KNNBasic
from surprise.model_selection import cross_validate

# Assuming ratings_df is already loaded with 'user_id', 'cocktail_id', and 'user_rating' columns
# Ensure the data types are correct
ratings_df['user_id'] = ratings_df['user_id'].astype(str)
ratings_df['cocktail_id'] = ratings_df['cocktail_id'].astype(str)
ratings_df['user_rating'] = ratings_df['user_rating'].astype(float)

# Define the reader with the appropriate rating scale
reader = Reader(rating_scale=(1, 5))

# Load the data into the Dataset object
data = Dataset.load_from_df(ratings_df[['user_id', 'cocktail_id', 'user_rating']], reader)

# Create a user-based collaborative filtering model
sim_options = {
    'name': 'cosine',
    'user_based': True  # Compute user similarity
}

# Initialize the KNNBasic algorithm
model = KNNBasic(sim_options=sim_options)

# Perform cross-validation
cross_validate(model, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)



import pandas as pd
from surprise import Dataset, Reader, accuracy
from surprise.model_selection import train_test_split
from surprise import KNNBasic

# # Assuming ratings_df is already loaded with 'user_id', 'cocktail_id', and 'user_rating' columns
# # Ensure the data types are correct
# ratings_df['user_id'] = ratings_df['user_id'].astype(str)
# ratings_df['cocktail_id'] = ratings_df['cocktail_id'].astype(str)
# ratings_df['user_rating'] = ratings_df['user_rating'].astype(float)

# # Define the reader with the appropriate rating scale
# reader = Reader(rating_scale=(1, 5))

# # Load the data into the Dataset object
# data = Dataset.load_from_df(ratings_df[['user_id', 'cocktail_id', 'user_rating']], reader)

# Split the data into a training set and a test set
trainset, testset = train_test_split(data, test_size=0.25, random_state=42)

# Create a user-based collaborative filtering model
sim_options = {
    'name': 'cosine',
    'user_based': True  # Compute user similarity
}

# Initialize the KNNBasic algorithm
model = KNNBasic(sim_options=sim_options)

# Fit the model on the training set
model.fit(trainset)

# Make predictions on the test set
predictions = model.test(testset)

# Calculate RMSE
rmse = accuracy.rmse(predictions)
print(f'RMSE: {rmse}')











import pandas as pd
from surprise import Dataset, Reader, SVD, accuracy
from surprise.model_selection import train_test_split as surprise_train_test_split

# Define the data reader
reader = Reader(rating_scale=(1, 5))

# Prepare training, validation, and test sets for the `surprise` library
train_data_surprise = Dataset.load_from_df(train_data[['user_id', 'cocktail_id', 'user_rating']], reader)
# train_data_surprise = Dataset.load_from_df(train_data_smote[['user_id', 'cocktail_id', 'user_rating']], reader)
# train_data_surprise = Dataset.load_from_df(train_data_rus[['user_id', 'cocktail_id', 'user_rating']], reader)
val_data_surprise = Dataset.load_from_df(val_data[['user_id', 'cocktail_id', 'user_rating']], reader)
test_data_surprise = Dataset.load_from_df(ratings_df_test[['user_id', 'cocktail_id', 'user_rating']], reader)

# # Build the full training set
trainset = train_data_surprise.build_full_trainset()

# Convert validation and test sets to the required format
valset = val_data_surprise.construct_testset(val_data_surprise.raw_ratings)
testset = test_data_surprise.construct_testset(test_data_surprise.raw_ratings)


from surprise import SVD, Dataset, Reader
from surprise.model_selection import cross_validate

# Train an SVD model
svd = SVD()
svd.fit(trainset)

# # Perform cross-validation
# cv_results = cross_validate(svd, trainset, measures=['RMSE', 'MAE'], cv=5, verbose=True)

# # Print the results
# print(cv_results)

# Evaluate the model on the validation set
val_predictions = svd.test(valset)
val_rmse = accuracy.rmse(val_predictions, verbose=True)

# # Final evaluation on the test set
# test_predictions = svd.test(testset)
# test_rmse = accuracy.rmse(test_predictions, verbose=True)

print(f"Validation RMSE: {val_rmse}")
# print(f"Test RMSE: {test_rmse}")


# # Final evaluation on the test set
# test_predictions = svd.test(testset)
# test_rmse = accuracy.rmse(test_predictions, verbose=True)

# # print(f"Validation RMSE: {val_rmse}")
# print(f"Test RMSE: {test_rmse}")








from surprise import SVD
from surprise.model_selection import RandomizedSearchCV
import numpy as np

# Define the parameter grid for random search
param_grid = {
    'n_factors': [20, 50, 100, 150],
    'n_epochs': [10, 20, 30, 40],
    'lr_all': [0.002, 0.005, 0.007, 0.01],
    'reg_all': [0.02, 0.05, 0.1, 0.2]
}

# Define the number of iterations for random search
n_iter = 20

# Perform Randomized Search
rs = RandomizedSearchCV(SVD, param_distributions=param_grid, measures=['rmse'], cv=10, n_iter=n_iter, random_state=42)
rs.fit(train_data_surprise)

# Get the best parameters
best_params = rs.best_params['rmse']
print("Best parameters: ", best_params)

# Use the best parameters to train the final model
best_svd = rs.best_estimator['rmse']
best_svd.fit(trainset)

# Evaluate on validation and test sets
val_predictions = best_svd.test(valset)
val_rmse = accuracy.rmse(val_predictions, verbose=True)

# test_predictions = best_svd.test(testset)
# test_rmse = accuracy.rmse(test_predictions, verbose=True)

print(f"Validation RMSE: {val_rmse}")
# print(f"Test RMSE: {test_rmse}")


# test_predictions = best_svd.test(testset)
# test_rmse = accuracy.rmse(test_predictions, verbose=True)

# # print(f"Validation RMSE: {val_rmse}")
# print(f"Test RMSE: {test_rmse}")





# Function to train SVD model and record RMSE at each epoch
def train_svd_with_learning_curves(trainset, valset, n_epochs, n_factors=100, lr_all=0.005, reg_all=0.02):
    train_errors = []
    val_errors = []
    
    svd = SVD(n_factors=n_factors, lr_all=lr_all, reg_all=reg_all, n_epochs=1)  # Initialize SVD with n_epochs=1 to control manually
    
    for epoch in range(n_epochs):
        svd.n_epochs = epoch + 1
        svd.fit(trainset)  # Incremental training
        
        # Calculate RMSE for training set
        train_predictions = svd.test(trainset.build_testset())
        train_rmse = accuracy.rmse(train_predictions, verbose=False)
        train_errors.append(train_rmse)
        
        # Calculate RMSE for validation set
        val_predictions = svd.test(valset)
        val_rmse = accuracy.rmse(val_predictions, verbose=False)
        val_errors.append(val_rmse)
        
        print(f"Epoch {epoch + 1}/{n_epochs} - Training RMSE: {train_rmse:.4f}, Validation RMSE: {val_rmse:.4f}")
    
    return train_errors, val_errors

# Train the model and record learning curves
n_epochs = 20
train_errors, val_errors = train_svd_with_learning_curves(trainset, valset, n_epochs)

# Plot the learning curves
def plot_learning_curves(train_errors, val_errors):
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, n_epochs + 1), train_errors, label='Training Error')
    plt.plot(range(1, n_epochs + 1), val_errors, label='Validation Error')
    plt.xlabel('Epochs')
    plt.ylabel('RMSE')
    plt.title('Learning Curves (Best SVD lr_all=0.005, reg_all=0.02)')
    plt.legend()
    plt.savefig(f'Learning Curves (Best SVD lr_all=0.005, reg_all=0.02).png')
    plt.show()

plot_learning_curves(train_errors, val_errors)


# The learning curves in the provided plot, along with the RMSE values over the epochs, provide insights into how the model is performing during training and on the validation set. Here's what the learning curves tell us:

# ### Analysis

# 1. **Training RMSE Decrease**:
#    - The training RMSE decreases steadily over the epochs, indicating that the model is learning and fitting the training data better with each epoch.
#    - By the 20th epoch, the training RMSE has significantly decreased to around 0.6805, showing that the model has learned the patterns in the training data well.

# 2. **Validation RMSE Increase**:
#    - The validation RMSE initially fluctuates slightly but then starts to increase after a few epochs.
#    - By the 20th epoch, the validation RMSE has increased to around 1.0387.

# ### Interpretation

# - **Overfitting**:
#   - The increasing gap between the training RMSE and validation RMSE indicates overfitting. The model is performing well on the training data but is not generalizing well to the validation data.
#   - The validation error increases despite the training error decreasing, which is a classic sign of overfitting.

# - **Model Complexity**:
#   - The model may be too complex for the data, capturing noise and specific patterns in the training data that do not generalize to unseen data.

# ### Recommendations

# To address overfitting, consider the following strategies:

# 1. **Increase Regularization**:
#    - Increase the regularization parameter (`reg_all`) to penalize large weights and reduce overfitting.
#    - For example, you might try increasing `reg_all` to 0.1 or 0.2.

# 2. **Early Stopping**:
#    - Use early stopping to halt training when the validation error starts to increase. This can prevent the model from overfitting to the training data.

# 3. **Reduce Model Complexity**:
#    - Reduce the number of latent factors (`n_factors`) to simplify the model and make it less prone to overfitting.

# 4. **Cross-Validation**:
#    - Use cross-validation to ensure the model's performance is consistent across different subsets of the data.

# 5. **Ensemble Methods**:
#    - Combine predictions from multiple models to reduce the impact of overfitting.

# ### Example of Adjusting Hyperparameters

# ```python
# # Train the model with increased regularization and reduced number of factors
# svd = SVD(n_factors=50, lr_all=0.005, reg_all=0.1, n_epochs=20)
# svd.fit(trainset)

# # Evaluate the model on the validation set
# val_predictions = svd.test(valset)
# val_rmse = accuracy.rmse(val_predictions, verbose=True)
# print(f"Validation RMSE after regularization adjustment: {val_rmse}")
# ```

# By adjusting these hyperparameters and potentially using early stopping, you can help mitigate overfitting and improve the model's generalization to the validation (and test) data.





# Function to train SVD model and record RMSE at each epoch using the best hyperparameters
def train_svd_with_learning_curves(trainset, valset, n_epochs, n_factors=50, lr_all=0.002, reg_all=0.05):
    train_errors = []
    val_errors = []
    
    svd = SVD(n_factors=n_factors, lr_all=lr_all, reg_all=reg_all, n_epochs=1)  # Initialize SVD with n_epochs=1 to control manually
    
    for epoch in range(n_epochs):
        svd.n_epochs = epoch + 1
        svd.fit(trainset)  # Incremental training
        
        # Calculate RMSE for training set
        train_predictions = svd.test(trainset.build_testset())
        train_rmse = accuracy.rmse(train_predictions, verbose=False)
        train_errors.append(train_rmse)
        
        # Calculate RMSE for validation set
        val_predictions = svd.test(valset)
        val_rmse = accuracy.rmse(val_predictions, verbose=False)
        val_errors.append(val_rmse)
        
        print(f"Epoch {epoch + 1}/{n_epochs} - Training RMSE: {train_rmse:.4f}, Validation RMSE: {val_rmse:.4f}")
    
    return train_errors, val_errors

# Train the model and record learning curves using the best hyperparameters
n_epochs = 20
train_errors, val_errors = train_svd_with_learning_curves(trainset, valset, n_epochs, n_factors=50, lr_all=0.002, reg_all=0.05)

# Plot the learning curves
def plot_learning_curves(train_errors, val_errors):
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, n_epochs + 1), train_errors, label='Training Error')
    plt.plot(range(1, n_epochs + 1), val_errors, label='Validation Error')
    plt.xlabel('Epochs')
    plt.ylabel('RMSE')
    plt.title('Learning Curves (Best SVD lr_all=0.002, reg_all=0.05)')
    plt.legend()
    plt.savefig(f'Learning Curves (Best SVD lr_all=0.002, reg_all=0.05).png')
    plt.show()

plot_learning_curves(train_errors, val_errors)









# Function to train SVD model and record RMSE at each epoch using the best hyperparameters
def train_svd_with_learning_curves(trainset, valset, n_epochs, n_factors=50, lr_all=0.002, reg_all=0.1):
    train_errors = []
    val_errors = []
    
    svd = SVD(n_factors=n_factors, lr_all=lr_all, reg_all=reg_all, n_epochs=1)  # Initialize SVD with n_epochs=1 to control manually
    
    for epoch in range(n_epochs):
        svd.n_epochs = epoch + 1
        svd.fit(trainset)  # Incremental training
        
        # Calculate RMSE for training set
        train_predictions = svd.test(trainset.build_testset())
        train_rmse = accuracy.rmse(train_predictions, verbose=False)
        train_errors.append(train_rmse)
        
        # Calculate RMSE for validation set
        val_predictions = svd.test(valset)
        val_rmse = accuracy.rmse(val_predictions, verbose=False)
        val_errors.append(val_rmse)
        
        print(f"Epoch {epoch + 1}/{n_epochs} - Training RMSE: {train_rmse:.4f}, Validation RMSE: {val_rmse:.4f}")
    
    return train_errors, val_errors

# Train the model and record learning curves using the best hyperparameters with increased regularization
n_epochs = 20
train_errors, val_errors = train_svd_with_learning_curves(trainset, valset, n_epochs, n_factors=50, lr_all=0.002, reg_all=0.1)

# Plot the learning curves
def plot_learning_curves(train_errors, val_errors):
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, n_epochs + 1), train_errors, label='Training Error')
    plt.plot(range(1, n_epochs + 1), val_errors, label='Validation Error')
    plt.xlabel('Epochs')
    plt.ylabel('RMSE')
    plt.title('Learning Curves (Best SVD lr_all=0.002, reg_all=0.1)')
    plt.legend()
    plt.savefig(f'Learning Curves (Best SVD lr_all=0.002, reg_all=0.1).png')
    plt.show()

plot_learning_curves(train_errors, val_errors)





# TESTING


# # Function to train SVD model and record RMSE at each epoch using the best hyperparameters
# def train_svd_with_learning_curves(trainset, valset, n_epochs, n_factors=20, lr_all=0.002, reg_all=0.2):
#     train_errors = []
#     val_errors = []
    
#     svd = SVD(n_factors=n_factors, lr_all=lr_all, reg_all=reg_all, n_epochs=1)  # Initialize SVD with n_epochs=1 to control manually
    
#     for epoch in range(n_epochs):
#         svd.n_epochs = epoch + 1
#         svd.fit(trainset)  # Incremental training
        
#         # Calculate RMSE for training set
#         train_predictions = svd.test(trainset.build_testset())
#         train_rmse = accuracy.rmse(train_predictions, verbose=False)
#         train_errors.append(train_rmse)
        
#         # Calculate RMSE for validation set
#         val_predictions = svd.test(valset)
#         val_rmse = accuracy.rmse(val_predictions, verbose=False)
#         val_errors.append(val_rmse)
        
#         print(f"Epoch {epoch + 1}/{n_epochs} - Training RMSE: {train_rmse:.4f}, Validation RMSE: {val_rmse:.4f}")
    
#     return train_errors, val_errors

# # Train the model and record learning curves using the best hyperparameters with increased regularization
# n_epochs = 20
# train_errors, val_errors = train_svd_with_learning_curves(trainset, valset, n_epochs, n_factors=50, lr_all=0.002, reg_all=0.1)

# # Plot the learning curves
# def plot_learning_curves(train_errors, val_errors):
#     plt.figure(figsize=(10, 6))
#     plt.plot(range(1, n_epochs + 1), train_errors, label='Training Error')
#     plt.plot(range(1, n_epochs + 1), val_errors, label='Validation Error')
#     plt.xlabel('Epochs')
#     plt.ylabel('RMSE')
#     plt.title('Learning Curves')
#     plt.legend()
#     plt.show()

# plot_learning_curves(train_errors, val_errors)








# def apply_smote(train_data):
#     smote = SMOTE(random_state=42)
#     X_train = train_data[['user_id', 'cocktail_id']]
#     y_train = train_data['user_rating']
#     X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
#     train_data_smote = pd.DataFrame(X_train_smote, columns=['user_id', 'cocktail_id'])
#     train_data_smote['user_rating'] = y_train_smote
#     return train_data_smote

# def apply_rus(train_data):
#     rus = RandomUnderSampler(random_state=42)
#     X_train = train_data[['user_id', 'cocktail_id']]
#     y_train = train_data['user_rating']
#     X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)
#     train_data_rus = pd.DataFrame(X_train_rus, columns=['user_id', 'cocktail_id'])
#     train_data_rus['user_rating'] = y_train_rus
#     return train_data_rus


# # def train_and_evaluate_svd(train_data, val_data, param_grid, n_iter=20):
# #     reader = Reader(rating_scale=(1, 5))
# #     train_data_surprise = Dataset.load_from_df(train_data[['user_id', 'cocktail_id', 'user_rating']], reader)
# #     # val_data_surprise = Dataset.load_from_df(val_data[['user_id', 'cocktail_id', 'user_rating']], reader)
    
# #     trainset = train_data_surprise.build_full_trainset()
# #     # valset = val_data_surprise.construct_testset(val_data_surprise.raw_ratings)
    
# #     rs = RandomizedSearchCV(SVD, param_distributions=param_grid, measures=['rmse'], cv=3, n_iter=n_iter, random_state=42)
# #     rs.fit(train_data_surprise)
    
# #     best_params = rs.best_params['rmse']
# #     best_svd = rs.best_estimator['rmse']
# #     best_svd.fit(trainset)
    
# #     val_predictions = best_svd.test(valset)
# #     val_rmse = accuracy.rmse(val_predictions, verbose=True)
    
# #     return best_svd, best_params, val_rmse

# # Function to train and evaluate SVD with RandomizedSearchCV
# def train_and_evaluate_svd(train_data, val_data, param_grid, n_iter=20):
#     reader = Reader(rating_scale=(1, 5))
#     train_data_surprise = Dataset.load_from_df(train_data[['user_id', 'cocktail_id', 'user_rating']], reader)
#     val_data_surprise = Dataset.load_from_df(val_data[['user_id', 'cocktail_id', 'user_rating']], reader)
    
#     trainset = train_data_surprise.build_full_trainset()
#     valset = val_data_surprise.construct_testset(val_data_surprise.raw_ratings)
    
#     # Perform Randomized Search
#     rs = RandomizedSearchCV(SVD, param_distributions=param_grid, n_iter=n_iter, cv=3, random_state=42, measures=['rmse'], refit=True)
#     rs.fit(train_data_surprise)
    
#     best_params = rs.best_params['rmse']
#     best_svd = rs.best_estimator['rmse']
#     best_svd.fit(trainset)
    
#     val_predictions = best_svd.test(valset)
#     val_rmse = accuracy.rmse(val_predictions, verbose=True)
    
#     return best_svd, best_params, val_rmse


# def plot_learning_curves(train_errors, val_errors, title, filename):
#     plt.figure(figsize=(10, 6))
#     plt.plot(range(1, len(train_errors) + 1), train_errors, label='Training Error')
#     plt.plot(range(1, len(val_errors) + 1), val_errors, label='Validation Error')
#     plt.xlabel('Epochs')
#     plt.ylabel('RMSE')
#     plt.title(title)
#     plt.legend()
#     plt.savefig(filename)
#     plt.show()


# def train_svd_with_learning_curves(trainset, valset, n_epochs, params):
#     train_errors = []
#     val_errors = []
#     svd = SVD(n_factors=params['n_factors'], lr_all=params['lr_all'], reg_all=params['reg_all'], n_epochs=1)
    
#     for epoch in range(n_epochs):
#         svd.n_epochs = epoch + 1
#         svd.fit(trainset)
        
#         train_predictions = svd.test(trainset.build_testset())
#         train_rmse = accuracy.rmse(train_predictions, verbose=False)
#         train_errors.append(train_rmse)
        
#         val_predictions = svd.test(valset)
#         val_rmse = accuracy.rmse(val_predictions, verbose=False)
#         val_errors.append(val_rmse)
        
#         print(f"Epoch {epoch + 1}/{n_epochs} - Training RMSE: {train_rmse:.4f}, Validation RMSE: {val_rmse:.4f}")
    
#     return train_errors, val_errors


# # Define parameter grid for random search
# param_grid = {
#     'n_factors': [20, 50, 100, 150],
#     'n_epochs': [10, 20, 30, 40],
#     'lr_all': [0.002, 0.005, 0.007, 0.01],
#     'reg_all': [0.02, 0.05, 0.1, 0.2]
# }

# # # Apply SMote and Random Undersampling
# # train_data_smote = apply_smote(train_data)
# # train_data_rus = apply_rus(train_data)

# # Store results
# results = []

# # Evaluate with original data
# print("Evaluating with Original Data:")
# best_svd_orig, best_params_orig, val_rmse_orig = train_and_evaluate_svd(train_data, val_data, param_grid)
# results.append(('Original', best_params_orig, val_rmse_orig))

# # Evaluate with SMOTE data
# print("Evaluating with SMOTE Data:")
# best_svd_smote, best_params_smote, val_rmse_smote = train_and_evaluate_svd(train_data_smote, val_data, param_grid)
# results.append(('SMOTE', best_params_smote, val_rmse_smote))

# # Evaluate with Random Undersampling data
# print("Evaluating with RUS Data:")
# best_svd_rus, best_params_rus, val_rmse_rus = train_and_evaluate_svd(train_data_rus, val_data, param_grid)
# results.append(('RUS', best_params_rus, val_rmse_rus))

# # Create a DataFrame to compare results
# results_df = pd.DataFrame(results, columns=['Method', 'Best Params', 'Validation RMSE'])
# print(results_df)



# results_df.to_csv('model_evaluation.csv', index = False)


# n_epochs = 50

# # Original Data
# print("\nTraining and plotting learning curves for Original Data:")
# train_data_surprise = Dataset.load_from_df(train_data[['user_id', 'cocktail_id', 'user_rating']], reader)
# trainset = train_data_surprise.build_full_trainset()
# valset = val_data_surprise.construct_testset(val_data_surprise.raw_ratings)
# train_errors, val_errors = train_svd_with_learning_curves(trainset, valset, n_epochs, best_params_orig)
# plot_learning_curves(train_errors, val_errors, "Learning Curves - Original Data", "learning_curves_original.png")

# # SMOTE Data
# print("\nTraining and plotting learning curves for SMOTE Data:")
# train_data_surprise = Dataset.load_from_df(train_data_smote[['user_id', 'cocktail_id', 'user_rating']], reader)
# trainset = train_data_surprise.build_full_trainset()
# valset = val_data_surprise.construct_testset(val_data_surprise.raw_ratings)
# train_errors, val_errors = train_svd_with_learning_curves(trainset, valset, n_epochs, best_params_smote)
# plot_learning_curves(train_errors, val_errors, "Learning Curves - SMOTE Data", "learning_curves_smote.png")

# # RUS Data
# print("\nTraining and plotting learning curves for RUS Data:")
# train_data_surprise = Dataset.load_from_df(train_data_rus[['user_id', 'cocktail_id', 'user_rating']], reader)
# trainset = train_data_surprise.build_full_trainset()
# valset = val_data_surprise.construct_testset(val_data_surprise.raw_ratings)
# train_errors, val_errors = train_svd_with_learning_curves(trainset, valset, n_epochs, best_params_rus)
# plot_learning_curves(train_errors, val_errors, "Learning Curves - RUS Data", "learning_curves_rus.png")





# def evaluate_on_test_data(model, test_data):
#     reader = Reader(rating_scale=(1, 5))
#     test_data_surprise = Dataset.load_from_df(test_data[['user_id', 'cocktail_id', 'user_rating']], reader)
#     testset = test_data_surprise.construct_testset(test_data_surprise.raw_ratings)
#     test_predictions = model.test(testset)
#     test_rmse = accuracy.rmse(test_predictions, verbose=True)
#     return test_rmse

# # Evaluate the best models on the test set
# print("\nEvaluating best models on the test set:")

# test_rmse_orig = evaluate_on_test_data(best_svd_orig, ratings_df_test)
# print(f"Test RMSE (Original): {test_rmse_orig}")

# test_rmse_smote = evaluate_on_test_data(best_svd_smote, ratings_df_test)
# print(f"Test RMSE (SMOTE): {test_rmse_smote}")

# test_rmse_rus = evaluate_on_test_data(best_svd_rus, ratings_df_test)
# print(f"Test RMSE (RUS): {test_rmse_rus}")


















# # Ideal curve should look like:
# import matplotlib.pyplot as plt
# import numpy as np

# epochs = np.arange(1, 21)
# train_error = np.exp(-0.2 * epochs) + 0.1
# val_error = np.exp(-0.2 * epochs) + 0.15

# plt.figure(figsize=(10, 6))
# plt.plot(epochs, train_error, label='Training Error')
# plt.plot(epochs, val_error, label='Validation Error')
# plt.xlabel('Epochs')
# plt.ylabel('Error')
# plt.title('Ideal Learning Curves')
# plt.legend()
# plt.show()





svd


best_svd
train_data





# # Get a list of all unique cocktails
# all_cocktails = cocktails_df['Name'].unique()

# num_cocktails_to_rec = 5

# # Function to get the top N recommendations for a given user
# def get_top_n_recommendations(model, user_id, all_cocktails, n=num_cocktails_to_rec):
#     # Get a list of cocktails the user has already rated
#     rated_cocktails = set(train_data[train_data['user_id'] == user_id]['cocktail_name'])
    
#     # Predict ratings for all cocktails the user hasn't rated yet
#     predictions = []
#     for cocktail in all_cocktails:
#         if cocktail not in rated_cocktails:
#             predictions.append((cocktail, model.predict(user_id, cocktail).est))
    
#     # Sort the predictions by estimated rating in descending order and get the top N recommendations
#     predictions.sort(key=lambda x: x[1], reverse=True)
#     top_n_recommendations = predictions[:n]
    
#     return top_n_recommendations

# # Function to get cocktails already rated by a given user
# def get_rated_cocktails(user_id, data):
#     rated_cocktails = data[data['user_id'] == user_id][['cocktail_name', 'user_rating']].sort_values('user_rating',ascending = False)
#     return rated_cocktails

# # Only consider user IDs that are present in the training data
# present_user_ids = train_data['user_id'].unique()

# # Get recommendations for users 1 through 5
# for user_id in present_user_ids[:3]:
#     rated_cocktails = get_rated_cocktails(user_id, train_data)
#     print(f"Cocktails rated by user {user_id}:")
#     print(rated_cocktails.head(5))
#     print()

#     recommendations = get_top_n_recommendations(svd, user_id, all_cocktails)
#     print(f"Top recommendations for user {user_id}:")
#     for cocktail, rating in recommendations:
#         print(f"  {cocktail}: {rating:.2f}")
#     print()

# ##################################

# # Create a DataFrame to store the recommendations
# recommendations_df = pd.DataFrame(columns=['user_id', 'cocktail_name', 'predicted_rating'])

# # Get recommendations for all users
# unique_user_ids = train_data['user_id'].unique()
# recommendations_list = []
# for user_id in unique_user_ids:
#     recommendations = get_top_n_recommendations(svd, user_id, all_cocktails)
#     for cocktail, rating in recommendations:
#         recommendations_list.append({'user_id': user_id, 'cocktail_name': cocktail, 'predicted_rating': rating})

# # Convert the list of recommendations to a DataFrame
# recommendations_df = pd.DataFrame(recommendations_list)

# # Save the recommendations to a CSV file
# recommendations_df.to_csv('top_5_recommendations_svd.csv', index=False)

# print("Top 5 recommendations for each user have been saved to 'top_5_recommendations_svd.csv'")

import pandas as pd

# Get a list of all unique cocktails
all_cocktails = cocktails_df['Name'].unique()
num_cocktails_to_rec = 5

# Function to get the top N recommendations for a given user
def get_top_n_recommendations(model, user_id, all_cocktails, n=num_cocktails_to_rec):
    # Get a list of cocktails the user has already rated
    rated_cocktails = set(train_data[train_data['user_id'] == user_id]['cocktail_name'])
    
    # Predict ratings for all cocktails the user hasn't rated yet
    predictions = []
    for cocktail in all_cocktails:
        if cocktail not in rated_cocktails:
            prediction = model.predict(user_id, cocktail)
            predictions.append((cocktail, prediction.est))
            # print(f"Predicted rating for {user_id} -> {cocktail}: {prediction.est}")  # Debug print
    
    # Sort the predictions by estimated rating in descending order and get the top N recommendations
    predictions.sort(key=lambda x: x[1], reverse=True)
    top_n_recommendations = predictions[:n]
    
    return top_n_recommendations

# Function to get cocktails already rated by a given user
def get_rated_cocktails(user_id, data):
    rated_cocktails = data[data['user_id'] == user_id][['cocktail_name', 'user_rating']].sort_values('user_rating', ascending=False)
    return rated_cocktails

# Only consider user IDs that are present in the training data
present_user_ids = train_data['user_id'].unique()

# Get recommendations for users who are in the training data
for user_id in present_user_ids[:3]:  # Adjust the range as needed
    rated_cocktails = get_rated_cocktails(user_id, train_data)
    print(f"Cocktails rated by user {user_id}:")
    print(rated_cocktails.head(5))
    print()

    recommendations = get_top_n_recommendations(svd, user_id, all_cocktails)
    print(f"Top recommendations for user {user_id}:")
    for cocktail, rating in recommendations:
        print(f"  {cocktail}: {rating:.2f}")
    print()

# Create a DataFrame to store the recommendations
recommendations_df = pd.DataFrame(columns=['user_id', 'cocktail_name', 'predicted_rating'])

# Get recommendations for all users
recommendations_list = []
for user_id in present_user_ids:
    recommendations = get_top_n_recommendations(svd, user_id, all_cocktails)
    for cocktail, rating in recommendations:
        recommendations_list.append({'user_id': user_id, 'cocktail_name': cocktail, 'predicted_rating': rating})

# Convert the list of recommendations to a DataFrame
recommendations_df = pd.DataFrame(recommendations_list)

# Save the recommendations to a CSV file
recommendations_df.to_csv('top_5_recommendations_svd.csv', index=False)

print("Top 5 recommendations for each user have been saved to 'top_5_recommendations_svd.csv'")






# Get a list of all unique cocktails
all_cocktails = cocktails_df['Name'].unique()

num_cocktails_to_rec = 5

# Function to get the top N recommendations for a given user
def get_top_n_recommendations(model, user_id, all_cocktails, n=num_cocktails_to_rec):
    # Get a list of cocktails the user has already rated
    rated_cocktails = set(train_data[train_data['user_id'] == user_id]['cocktail_name'])
    
    # Predict ratings for all cocktails the user hasn't rated yet
    predictions = []
    for cocktail in all_cocktails:
        if cocktail not in rated_cocktails:
            predictions.append((cocktail, model.predict(user_id, cocktail).est))
    
    # Sort the predictions by estimated rating in descending order and get the top N recommendations
    predictions.sort(key=lambda x: x[1], reverse=True)
    top_n_recommendations = predictions[:n]
    
    return top_n_recommendations

# Function to get cocktails already rated by a given user
def get_rated_cocktails(user_id, data):
    rated_cocktails = data[data['user_id'] == user_id][['cocktail_name', 'user_rating']].sort_values('user_rating',ascending = False)
    return rated_cocktails

# Only consider user IDs that are present in the training data
present_user_ids = train_data['user_id'].unique()

# Get recommendations for users 1 through 5
for user_id in present_user_ids[:3]:
    rated_cocktails = get_rated_cocktails(user_id, train_data)
    print(f"Cocktails rated by user {user_id}:")
    print(rated_cocktails.head(5))
    print()

    recommendations = get_top_n_recommendations(best_svd, user_id, all_cocktails)
    print(f"Top recommendations for user {user_id}:")
    for cocktail, rating in recommendations:
        print(f"  {cocktail}: {rating:.2f}")
    print()


##################################

# Create a DataFrame to store the recommendations
recommendations_df = pd.DataFrame(columns=['user_id', 'cocktail_name', 'predicted_rating'])

# Get recommendations for all users
unique_user_ids = train_data['user_id'].unique()
recommendations_list = []
for user_id in unique_user_ids:
    recommendations = get_top_n_recommendations(best_svd, user_id, all_cocktails)
    for cocktail, rating in recommendations:
        recommendations_list.append({'user_id': user_id, 'cocktail_name': cocktail, 'predicted_rating': rating})

# Convert the list of recommendations to a DataFrame
recommendations_df = pd.DataFrame(recommendations_list)

# Save the recommendations to a CSV file
recommendations_df.to_csv('top_5_recommendations_best_svd.csv', index=False)

print("Top 5 recommendations for each user have been saved to 'top_5_recommendations_best_svd.csv'")











# from pptx import Presentation
# from pptx.util import Inches

# # Create a presentation object
# prs = Presentation()

# # Slide 1: Title Slide
# slide_layout = prs.slide_layouts[0]
# slide = prs.slides.add_slide(slide_layout)
# title = slide.shapes.title
# subtitle = slide.placeholders[1]
# title.text = "Cocktails Recommendation System"
# subtitle.text = "Using Machine Learning to Suggest Cocktails\nPresented By: [Your Name]\nDate: [Today's Date]"

# # Slide 2: Problem Statement
# slide_layout = prs.slide_layouts[1]
# slide = prs.slides.add_slide(slide_layout)
# title, content = slide.shapes.title, slide.placeholders[1]
# title.text = "Problem Statement"
# content.text = "- Briefly describe the problem.\n- Importance of a recommendation system for cocktails.\n- Objective of the project."

# # Slide 3: Assumptions and Hypotheses
# slide = prs.slides.add_slide(slide_layout)
# title, content = slide.shapes.title, slide.placeholders[1]
# title.text = "Assumptions and Hypotheses"
# content.text = "- Assumptions about user preferences.\n- Hypotheses about how users' ratings are distributed.\n- Expected outcome from the recommendation model."

# # Slide 4: Exploratory Data Analysis
# slide = prs.slides.add_slide(slide_layout)
# title, content = slide.shapes.title, slide.placeholders[1]
# title.text = "Exploratory Data Analysis"
# content.text = "- Initial observations from the data.\n- Visualizations (distributions, correlations, etc.).\n- Key insights from the data."

# # Slide 5: Feature Engineering & Transformations
# slide = prs.slides.add_slide(slide_layout)
# title, content = slide.shapes.title, slide.placeholders[1]
# title.text = "Feature Engineering & Transformations"
# content.text = "- Description of feature engineering steps.\n- Transformations applied to the data.\n- Rationale behind the chosen features and transformations."

# # Slide 6: Proposed Approaches (Model)
# slide = prs.slides.add_slide(slide_layout)
# title, content = slide.shapes.title, slide.placeholders[1]
# title.text = "Proposed Approaches (Model)"
# content.text = "- Description of the models tried (SVD, RandomForest, etc.).\n- Cross-validation techniques used.\n- Checks for overfitting and underfitting."

# # Slide 7: Model Selection
# slide = prs.slides.add_slide(slide_layout)
# title, content = slide.shapes.title, slide.placeholders[1]
# title.text = "Model Selection"
# content.text = "- Final model chosen.\n- Regularization techniques applied.\n- Justification for the model choice."

# # Slide 8: Results and Learnings
# slide = prs.slides.add_slide(slide_layout)
# title, content = slide.shapes.title, slide.placeholders[1]
# title.text = "Results and Learnings"
# content.text = "- Accuracy metrics (RMSE, MAE, etc.).\n- Learning curves.\n- Key findings and learnings from the project."

# # Slide 9: Future Work
# slide = prs.slides.add_slide(slide_layout)
# title, content = slide.shapes.title, slide.placeholders[1]
# title.text = "Future Work"
# content.text = "- Potential improvements.\n- Future directions for the project.\n- Other models or techniques to explore."

# # Slide 10: Questions & Answers
# slide = prs.slides.add_slide(slide_layout)
# title, content = slide.shapes.title, slide.placeholders[1]
# title.text = "Questions & Answers"
# content.text = "Open floor for questions.\nThank the audience for their attention."

# # Save the presentation
# prs.save('Cocktails_Recommendation_Presentation.pptx')



# from pptx import Presentation
# from pptx.util import Inches

# # Create a presentation object
# prs = Presentation()

# # Add a title slide
# slide = prs.slides.add_slide(prs.slide_layouts[0])
# title = slide.shapes.title
# subtitle = slide.placeholders[1]
# title.text = "Cocktail Recommendation System"
# subtitle.text = "A Machine Learning Project\nYour Name\nDate"

# # Add slides for each section
# sections = [
#     "Problem Statement", "Assumptions and Hypotheses", "Exploratory Data Analysis",
#     "Feature Engineering", "Model Approaches", "Model Selection",
#     "Results", "Learnings", "Future Work"
# ]

# for section in sections:
#     slide = prs.slides.add_slide(prs.slide_layouts[1])
#     title = slide.shapes.title
#     content = slide.placeholders[1]
#     title.text = section
#     content.text = f"Content for {section} slide."

# # Save the presentation
# prs.save('Cocktail_Recommendation_System.pptx')








# Deep Learning


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate
from tensorflow.keras.optimizers import Adam

# Assuming the same fake ratings dataframe from the earlier example
# For demonstration, we're reusing the 'ratings_df' created earlier

# Encoding user IDs and cocktail names
user_encoder = LabelEncoder()
item_encoder = LabelEncoder()

ratings_df['user_id_encoded'] = user_encoder.fit_transform(ratings_df['user_id'])
ratings_df['item_id_encoded'] = item_encoder.fit_transform(ratings_df['cocktail_id'])

# Splitting the data
X = ratings_df[['user_id_encoded', 'item_id_encoded']]
y = ratings_df['user_rating']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Neural network architecture
user_input = Input(shape=(1,), name='user_input')
item_input = Input(shape=(1,), name='item_input')

user_embedding = Embedding(output_dim=5, input_dim=len(user_encoder.classes_), name='user_embedding')(user_input)
item_embedding = Embedding(output_dim=5, input_dim=len(item_encoder.classes_), name='item_embedding')(item_input)

user_vec = Flatten(name='flatten_users')(user_embedding)
item_vec = Flatten(name='flatten_items')(item_embedding)

concat = Concatenate()([user_vec, item_vec])
fc1 = Dense(128, activation='relu')(concat)
output = Dense(1)(fc1)

model = Model([user_input, item_input], output)

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit([X_train['user_id_encoded'], X_train['item_id_encoded']], y_train, epochs=10, verbose=1, validation_split=0.1)

# Evaluate the model
model.evaluate([X_test['user_id_encoded'], X_test['item_id_encoded']], y_test)



def make_recommendations(user_id, num_recommendations=5):
    user_idx = user_encoder.transform([user_id])[0]
    all_cocktails_idx = np.array(list(set(ratings_df['item_id_encoded'])))
    user_idx_array = np.array([user_idx for _ in range(len(all_cocktails_idx))])
    predictions = model.predict([user_idx_array, all_cocktails_idx])
    top_ratings_indices = predictions.flatten().argsort()[-num_recommendations:][::-1]
    top_cocktail_ids = item_encoder.inverse_transform(all_cocktails_idx[top_ratings_indices])
    return top_cocktail_ids

# Test recommendations for a user
print("Top recommendations for a user:", make_recommendations('User5'))


def make_batch_recommendations(user_ids, num_recommendations=5):
    # Transform user_ids to indices
    user_idx = user_encoder.transform(user_ids)
    # Get all cocktails indices
    all_cocktails_idx = np.array(list(set(ratings_df['item_id_encoded'])))
    # Prepare the batch data
    user_idx_array = np.tile(user_idx[:, np.newaxis], (1, len(all_cocktails_idx))).flatten()
    all_cocktails_idx_array = np.tile(all_cocktails_idx, len(user_idx))
    
    # Predict ratings for all combinations
    predictions = model.predict([user_idx_array, all_cocktails_idx_array]).flatten()
    # Reshape predictions to match the number of users and items
    predictions_matrix = predictions.reshape(len(user_idx), len(all_cocktails_idx))
    
    # Get the top recommendations for each user
    top_ratings_indices = np.argsort(predictions_matrix, axis=1)[:, -num_recommendations:]
    
    # Iterate over each user's top indices and decode the cocktail names
    recommendations = {}
    for i, indices in enumerate(top_ratings_indices):
        top_cocktail_ids = item_encoder.inverse_transform(all_cocktails_idx[indices])
        recommendations[user_ids[i]] = top_cocktail_ids

    return recommendations

# Example: Recommend cocktails for multiple users
user_list = ['User1', 'User2', 'User3']
recommendations = make_batch_recommendations(user_list)
for user, recs in recommendations.items():
    print(f"Top recommendations for {user}: {recs}")


ratings_df


import seaborn as sns
from sklearn.decomposition import PCA

# Use PCA to reduce dimensionality of embeddings for visualization
def visualize_embeddings(embeddings, labels):
    pca = PCA(n_components=2)
    reduced = pca.fit_transform(embeddings)
    
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=reduced[:, 0], y=reduced[:, 1], hue=labels)
    plt.title('PCA on Embeddings')
    plt.show()

# Visualizing user and item embeddings
user_embeddings = model.get_layer('user_embedding').get_weights()[0]
item_embeddings = model.get_layer('item_embedding').get_weights()[0]
visualize_embeddings(user_embeddings, user_encoder.inverse_transform(range(len(user_encoder.classes_))))
visualize_embeddings(item_embeddings, item_encoder.inverse_transform(range(len(item_encoder.classes_))))



import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Function to plot embeddings using t-SNE
def plot_embeddings(embeddings, names, title):
    tsne = TSNE(n_components=2, random_state=42)
    components = tsne.fit_transform(embeddings)
    plt.figure(figsize=(10, 6))
    plt.scatter(components[:, 0], components[:, 1])
    for i, name in enumerate(names):
        plt.annotate(name, (components[i, 0], components[i, 1]))
    plt.title(title)
    plt.grid(True)
    plt.show()

# Extract embeddings
user_embeddings = model.get_layer('user_embedding').get_weights()[0]
item_embeddings = model.get_layer('item_embedding').get_weights()[0]

# Get names for annotations
user_names = user_encoder.inverse_transform(range(len(user_encoder.classes_)))
item_names = item_encoder.inverse_transform(range(len(item_encoder.classes_)))

# Plotting
plot_embeddings(user_embeddings, user_names, 'User Embeddings')
plot_embeddings(item_embeddings, item_names, 'Item (Cocktail) Embeddings')



# Analyze the dense layer weights
fc1_weights = model.get_layer('dense').get_weights()[0]

# Sum absolute weights to see which input features (post-embedding) have the most influence
feature_importance = np.sum(np.abs(fc1_weights), axis=1)

# Plot feature importance
plt.bar(range(len(feature_importance)), feature_importance)
plt.title('Feature Importance in Dense Layer')
plt.xlabel('Feature Index')
plt.ylabel('Sum of Absolute Weights')
plt.show()





# Here's an example implementation of NCF using TensorFlow/Keras:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Load the dataset
cocktails_file_path = 'cocktails.csv'
cocktails_df = pd.read_csv(cocktails_file_path)

# Replace all cells with '-' with null (None)
cocktails_df.replace('-', None, inplace=True)

# Label encode categorical columns
label_encoders = {}
categorical_cols = cocktails_df.select_dtypes(include=['object']).columns

for col in categorical_cols:
    le = LabelEncoder()
    cocktails_df[col] = le.fit_transform(cocktails_df[col].astype(str))
    label_encoders[col] = le

# Generate synthetic user ratings data
def generate_synthetic_ratings(num_records):
    user_ids = np.random.randint(1, 1001, num_records)
    cocktail_names = np.random.choice(cocktails_df['Name'].unique(), num_records)
    user_ratings = np.random.randint(1, 6, num_records)
    synthetic_data = pd.DataFrame({
        'user_id': user_ids,
        'cocktail_name': cocktail_names,
        'user_rating': user_ratings
    })
    return synthetic_data

num_records = 1000  # Change this number to generate different sizes of datasets
ratings_df = generate_synthetic_ratings(num_records)

# Create a mapping from cocktail names to indices
cocktail_map = {name: idx for idx, name in enumerate(cocktails_df['Name'].unique())}

# Add a column for cocktail indices
ratings_df['cocktail_idx'] = ratings_df['cocktail_name'].map(cocktail_map)

# Ensure user_id and cocktail_idx are within valid range
max_user_id = ratings_df['user_id'].max()
max_cocktail_idx = ratings_df['cocktail_idx'].max()

num_users = max_user_id + 1
num_cocktails = max_cocktail_idx + 1

# Define the NCF model
def create_ncf_model(num_users, num_cocktails, embedding_dim=50, layers=[128, 64, 32]):
    # Input layers
    user_input = Input(shape=(1,), name='user_input')
    item_input = Input(shape=(1,), name='item_input')
    
    # Embedding layers
    user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_embedding')(user_input)
    item_embedding = Embedding(input_dim=num_cocktails, output_dim=embedding_dim, name='item_embedding')(item_input)
    
    # Flatten the embeddings
    user_flatten = Flatten()(user_embedding)
    item_flatten = Flatten()(item_embedding)
    
    # Concatenate user and item embeddings
    concatenated = Concatenate()([user_flatten, item_flatten])
    
    # Fully connected layers
    x = concatenated
    for layer_size in layers:
        x = Dense(layer_size, activation='relu')(x)
        x = Dropout(0.2)(x)
    
    # Output layer
    output = Dense(1, activation='sigmoid')(x)
    
    # Create the model
    model = Model(inputs=[user_input, item_input], outputs=output)
    return model

# Create the NCF model
embedding_dim = 50
layers = [128, 64, 32]
ncf_model = create_ncf_model(num_users, num_cocktails, embedding_dim, layers)

# Compile the model
ncf_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['accuracy'])

# Prepare data for training
X = [ratings_df['user_id'].values, ratings_df['cocktail_idx'].values]
y = ratings_df['user_rating'].values

# Split data into train and test sets
user_train, user_test, cocktail_train, cocktail_test, y_train, y_test = train_test_split(X[0], X[1], y, test_size=0.2, random_state=42)

# Train the model
ncf_model.fit([user_train, cocktail_train], y_train, epochs=10, batch_size=64, validation_data=([user_test, cocktail_test], y_test))

# Evaluate the model
loss, accuracy = ncf_model.evaluate([user_test, cocktail_test], y_test)
print(f"Test Accuracy: {accuracy}")






# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import LabelEncoder
# from sklearn.model_selection import train_test_split
# import tensorflow as tf
# from tensorflow.keras.models import Model
# from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate
# from tensorflow.keras.optimizers import Adam

# # # Load the datasets
# # cocktails_df = pd.read_csv('path_to_cocktails_df.csv')
# # ratings_df = pd.read_csv('path_to_ratings_df.csv')

# # Preprocess the data
# user_encoder = LabelEncoder()
# item_encoder = LabelEncoder()

# ratings_df['user_id_encoded'] = user_encoder.fit_transform(ratings_df['user_id'])
# ratings_df['item_id_encoded'] = item_encoder.fit_transform(ratings_df['cocktail_name'])

# num_users = ratings_df['user_id_encoded'].nunique()
# num_items = ratings_df['item_id_encoded'].nunique()

# # Split the data into train and test sets
# train_data, test_data = train_test_split(ratings_df, test_size=0.2, random_state=42)

# # Define the model architecture
# embedding_size = 50

# # User and item inputs
# user_input = Input(shape=(1,), name='user_input')
# item_input = Input(shape=(1,), name='item_input')

# # User and item embeddings
# user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size, name='user_embedding')(user_input)
# item_embedding = Embedding(input_dim=num_items, output_dim=embedding_size, name='item_embedding')(item_input)

# # Flatten the embeddings
# user_vec = Flatten(name='user_flatten')(user_embedding)
# item_vec = Flatten(name='item_flatten')(item_embedding)

# # Concatenate user and item embeddings
# concat = Concatenate(name='concat')([user_vec, item_vec])

# # Add dense layers
# dense1 = Dense(128, activation='relu')(concat)
# dense2 = Dense(64, activation='relu')(dense1)
# output = Dense(1, activation='linear')(dense2)

# # Build the model
# model = Model(inputs=[user_input, item_input], outputs=output)
# model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

# # Train the model
# history = model.fit(
#     [train_data['user_id_encoded'], train_data['item_id_encoded']],
#     train_data['user_rating'],
#     batch_size=64,
#     epochs=20,
#     validation_data=(
#         [test_data['user_id_encoded'], test_data['item_id_encoded']],
#         test_data['user_rating']
#     )
# )

# # Evaluate the model
# test_loss = model.evaluate(
#     [test_data['user_id_encoded'], test_data['item_id_encoded']],
#     test_data['user_rating']
# )
# print(f'Test Loss: {test_loss}')

# # Function to make batch recommendations
# def make_batch_recommendations(user_ids, num_recommendations=5):
#     user_idx = user_encoder.transform(user_ids)
#     all_cocktails_idx = np.array(list(set(ratings_df['item_id_encoded'])))
#     user_idx_array = np.tile(user_idx[:, np.newaxis], (1, len(all_cocktails_idx))).flatten()
#     all_cocktails_idx_array = np.tile(all_cocktails_idx, len(user_idx))
#     predictions = model.predict([user_idx_array, all_cocktails_idx_array]).flatten()
#     predictions_matrix = predictions.reshape(len(user_idx), len(all_cocktails_idx))
#     top_ratings_indices = np.argsort(predictions_matrix, axis=1)[:, -num_recommendations:]
#     recommendations = {}
#     for i, indices in enumerate(top_ratings_indices):
#         top_cocktail_ids = item_encoder.inverse_transform(all_cocktails_idx[indices])
#         recommendations[user_ids[i]] = top_cocktail_ids
#     return recommendations

# # Example: Recommend cocktails for multiple users
# user_list = ['user1@gmail.com', 'user2@gmail.com', 'user3@gmail.com']
# recommendations = make_batch_recommendations(user_list)
# for user, recs in recommendations.items():
#     print(f"Top recommendations for {user}: {recs}")



ratings_df


cocktails_df


import pandas as pd
import numpy as np

# Assuming cocktails_df and ratings_df are already loaded in the following format
# cocktails_df: DataFrame with columns ['cocktail_id', 'Name', 'Category', ...]
# ratings_df: DataFrame with columns ['user_id', 'cocktail_id', 'user_rating']

# Pivot the DataFrame to create a user-item matrix
R_df = ratings_df.pivot(index='user_id', columns='cocktail_id', values='user_rating').fillna(0)
R = R_df.values

# Normalize (demean) the data
user_ratings_mean = np.mean(R, axis=1)
R_demeaned = R - user_ratings_mean.reshape(-1, 1)

# Perform SVD
from scipy.sparse.linalg import svds
U, sigma, Vt = svds(R_demeaned, k=50)
sigma = np.diag(sigma)

# Calculate predicted ratings
all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)
preds_df = pd.DataFrame(all_user_predicted_ratings, columns=R_df.columns)


def recommend_cocktails(predictions_df, user_id, cocktails_df, original_ratings_df, num_recommendations=5):
    # Get and sort the user's predictions
    user_row_number = user_id - 1  # UserID starts at 1, not 0
    sorted_user_predictions = predictions_df.iloc[user_row_number].sort_values(ascending=False)  # UserID starts at 1
    
    # Get the user's data and merge in the cocktail information
    user_data = original_ratings_df[original_ratings_df.user_id == user_id]
    user_full = (user_data.merge(cocktails_df, how='left', left_on='cocktail_id', right_on='cocktail_id')
                 .sort_values(['user_rating'], ascending=False))

    # Recommend the highest predicted rating cocktails that the user hasn't seen yet
    recommendations = (cocktails_df[~cocktails_df['cocktail_id'].isin(user_full['cocktail_id'])]
                       .merge(pd.DataFrame(sorted_user_predictions).reset_index(), how='left',
                              left_on='cocktail_id', right_on='cocktail_id')
                       .rename(columns={user_row_number: 'Predictions'})
                       .sort_values('Predictions', ascending=False)
                       .iloc[:num_recommendations, :-1])
    
    return user_full, recommendations

# Example usage:
already_rated, predictions = recommend_cocktails(preds_df, 8377205875, cocktails_df, ratings_df, 10)

print("Cocktails already rated by user:")
print(already_rated.head(10))

print("\nTop 10 cocktail recommendations for the user:")
print(predictions)



